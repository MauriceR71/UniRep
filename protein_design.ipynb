{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cszfTJ_Xa7Xj"
   },
   "source": [
    "## How to use the UniRep mLSTM \"babbler\". This version demonstrates the 64-unit and the 1900-unit architecture. \n",
    "\n",
    "We recommend getting started with the 64-unit architecture as it is easier and faster to run, but has the same interface as the 1900-unit one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUDzostda7Xx"
   },
   "source": [
    "Use the 64-unit or the 1900-unit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XO3UpPHja7X2"
   },
   "outputs": [],
   "source": [
    "USE_FULL_1900_DIM_MODEL = False # if True use 1900 dimensional model, else use 64 dimensional one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVmnJZg0a7YK"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZKtNtge6a7YO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if USE_FULL_1900_DIM_MODEL:\n",
    "    # Sync relevant weight files\n",
    "    !aws s3 sync --no-sign-request --quiet s3://unirep-public/1900_weights/ 1900_weights/\n",
    "    \n",
    "    # Import the mLSTM babbler model\n",
    "    from unirep import babbler1900 as babbler\n",
    "    \n",
    "    # Where model weights are stored.\n",
    "    MODEL_WEIGHT_PATH = \"./1900_weights\"\n",
    "    \n",
    "else:\n",
    "    # Sync relevant weight files\n",
    "    !aws s3 sync --no-sign-request --quiet s3://unirep-public/64_weights/ 64_weights/\n",
    "    \n",
    "    # Import the mLSTM babbler model\n",
    "    from unirep import babbler64 as babbler\n",
    "    \n",
    "    # Where model weights are stored.\n",
    "    MODEL_WEIGHT_PATH = \"./64_weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrzHUSvUa7Ym"
   },
   "source": [
    "## Data formatting and management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfGVLDfMa7Y0"
   },
   "source": [
    "Initialize UniRep, also referred to as the \"babbler\" in our code. You need to provide the batch size you will use and the path to the weight directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MizsFF4a7Y3",
    "outputId": "84924bb6-60c3-400b-c4c9-b647aff20729"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "b = babbler(batch_size=batch_size, model_path=MODEL_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wR0PeUmua7ZC"
   },
   "source": [
    "UniRep needs to receive data in the correct format, a (batch_size, max_seq_len) matrix with integer values, where the integers correspond to an amino acid label at that position, and the end of the sequence is padded with 0s until the max sequence length to form a non-ragged rectangular matrix. We provide a formatting function to translate a string of amino acids into a list of integers with the correct codex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZrXCYLvLa7ZM"
   },
   "outputs": [],
   "source": [
    "seq = \"MRKGEELFTGVVPILVELDGDVNGHKFSVRGFKSAMPEGYVQERTISFKDDGTYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNFNSHNVYITADKQKNGIKANFKIRHNVEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSVLSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_WY4lAja7ZX",
    "outputId": "3f5d7179-528a-4180-bf64-33f81a8846d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24,  1,  2,  4, 13,  6,  6, 21, 18,  8, 13, 16, 16, 14, 17, 21, 16,\n",
       "        6, 21,  5, 13,  5, 16,  9, 13,  3,  4, 18,  7, 16,  2, 13, 18,  4,\n",
       "        7, 15,  1, 14,  6, 13, 19, 16, 10,  6,  2,  8, 17,  7, 18,  4,  5,\n",
       "        5, 13,  8, 19,  4,  8,  2, 15,  6, 16,  4, 18,  6, 13,  5,  8, 21,\n",
       "       16,  9,  2, 17,  6, 21,  4, 13, 17,  5, 18,  4,  6,  5, 13,  9, 17,\n",
       "       21, 13,  3,  4, 21,  6, 19,  9, 18,  9,  7,  3,  9, 16, 19, 17,  8,\n",
       "       15,  5,  4, 10,  4,  9, 13, 17,  4, 15,  9, 18,  4, 17,  2,  3,  9,\n",
       "       16,  6,  5, 13,  7, 16, 10, 21, 15,  5,  3, 19, 10, 10,  9,  8, 14,\n",
       "       17, 13,  5, 13, 14, 16, 21, 21, 14,  5,  9,  3, 19, 21,  7,  8, 10,\n",
       "        7, 16, 21,  7,  4,  5, 14,  9,  6,  4,  2,  5,  3,  1, 16, 21, 21,\n",
       "        6, 18, 16,  8, 15, 15, 13, 17,  8,  3, 13,  1,  5,  6, 21, 19,  4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(b.format_seq(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ohu5ct2a7Ze"
   },
   "source": [
    "We also provide a function that will check your amino acid sequences don't contain any characters which will break the UniRep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-HdicVaa7Zs",
    "outputId": "7ab56007-8799-4193-bf5f-afe0e825aea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.is_valid_seq(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKgWRzWwOZnd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.1467146e-02  8.5657609e-01 -3.3489746e-01 -9.0909624e+00\n",
      " -5.1293910e-02 -7.9396874e-01  1.2415814e+00  1.3914506e+00\n",
      "  4.2770794e-01 -9.1309536e-01  7.3714471e-01  3.7204704e+00\n",
      " -6.1502516e-02  3.6691222e-02 -5.3945884e-02 -5.6928349e-01\n",
      " -8.8524551e+00  6.2627241e-02  1.7972789e+00 -4.4495778e+00\n",
      " -3.1581536e-01  2.1176586e+00 -1.6655995e+00 -3.0214908e+00\n",
      "  5.4951391e+00 -1.0015080e+00 -2.7671546e-01  1.5090264e-01\n",
      "  6.7902198e+00 -1.1953800e+01  5.2513447e+00  5.0253266e-01\n",
      "  1.6759138e+00 -7.7483821e-01  1.5462296e+00  3.3909041e-01\n",
      " -6.6190827e-01  3.6993891e-01  2.9829463e-02 -5.7673329e-01\n",
      "  2.1338558e-02  5.3747725e-01 -4.6976127e-02  7.2446221e-01\n",
      "  1.1045490e-01  2.6462986e+00 -2.1396317e+00 -1.6925581e+01\n",
      "  1.5338727e+00  8.3889019e-01  7.0084685e-01  1.8553182e-01\n",
      " -2.1598063e-01 -2.4932203e-01  1.6302848e-01  6.0178697e-02\n",
      " -1.1138448e-01 -2.4317503e-02  2.5652668e-02  1.0783783e+01\n",
      " -4.2227337e-01 -3.4802705e-02  3.2718506e+00  4.0632207e-03]\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "rep = b.get_rep(seq)\n",
    "print(rep[-1])\n",
    "print(len(rep[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oliLJORgNwCi"
   },
   "outputs": [],
   "source": [
    "b.get_babble(\"MRKGEEL\", length=290, temp=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zefLdj0fZr1T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 seqs processed...\n",
      "25 seqs processed...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6d76402c9b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/hydrolase_vals.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mvectorize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-6d76402c9b58>\u001b[0m in \u001b[0;36mvectorize_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seqs processed...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mseq_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mseq_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/hydrolase_seqvecs.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/unirep.py\u001b[0m in \u001b[0;36mget_rep\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    679\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minibatch_x_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                     self._initial_state_placeholder: self._zero_state}\n\u001b[0m\u001b[1;32m    682\u001b[0m             )\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unirep import babbler64 as babbler\n",
    "\n",
    "\n",
    "def get_data(filename):\n",
    "    df_pet = pd.read_csv(filename)\n",
    "    seqs = df_pet[\"aaSequence\"]\n",
    "    vals = df_pet[\"medianBrightness\"]\n",
    "    vals = vals.astype(np.float)\n",
    "    return seqs, vals\n",
    "\n",
    "\n",
    "# Turns each sequence into a np array\n",
    "def vectorize_data():\n",
    "    seqs, vals = get_data(\"data/PETase_mutations_1.csv\")\n",
    "    seq_vecs = []\n",
    "    for seq in seqs:\n",
    "        seq = seq.upper()\n",
    "        seq_vecs.append(b.get_rep(seq))\n",
    "    seq_vecs = np.array(seq_vecs)\n",
    "    np.save(\"data/petase_seqvecs.npy\", seq_vecs)\n",
    "    np.save(\"data/petase_vals.npy\", vals)\n",
    "\n",
    "    seqs, vals = get_data(\"data/hydrolase.csv\")\n",
    "    seq_vecs = []\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if i % 25 == 0:\n",
    "            print(i, \"seqs processed...\")\n",
    "        seq = seq.upper()\n",
    "        seq_vecs.append(b.get_rep(seq))\n",
    "    seq_vecs = np.array(seq_vecs)\n",
    "    np.save(\"data/hydrolase_seqvecs.npy\", seq_vecs)\n",
    "    np.save(\"data/hydrolase_vals.npy\", vals)\n",
    "\n",
    "vectorize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxx4Qv5AUNS1"
   },
   "source": [
    "# Instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "amVBw4hFUPAD"
   },
   "outputs": [],
   "source": [
    "AMINO_ACIDS = ['W', 'C', 'M', 'H', 'Y', 'F', 'Q', 'N', 'I', 'R', 'D', 'P', 'T',\n",
    "               'K', 'E', 'V', 'S', 'G', 'A', 'L']\n",
    "\n",
    "def construct_diwv():\n",
    "  diwv_dict = {}\n",
    "  matrix = []\n",
    "  with open(\"./diwv.csv\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      scores = line.split(\",\")\n",
    "      matrix.append([float(s) for s in scores])\n",
    "  for i in range(20):\n",
    "    subdict = {}\n",
    "    for j in range(20):\n",
    "      subdict[AMINO_ACIDS[j]] = matrix[i][j]\n",
    "    diwv_dict[AMINO_ACIDS[i]] = subdict\n",
    "  return diwv_dict\n",
    "  \n",
    "diwv_dict = construct_diwv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qjojn1PZQJWH"
   },
   "source": [
    "# Karpathy RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cEWLgYcWQGeA"
   },
   "outputs": [],
   "source": [
    "from oracles import oracle\n",
    "\n",
    "# Returns True iff protein sequence passes preset filters\n",
    "def passes_filters(seq, min_len=270, max_len=310,\n",
    "                   max_instability_index=40,\n",
    "                   max_conserved_residue_penalty=20,\n",
    "                   oracle_name='deepnn'):\n",
    "\n",
    "  # controls sequence length\n",
    "  if len(seq) <= 270 or len(seq) >= 310:\n",
    "    return False\n",
    "  \n",
    "  # controls sequence stability (ProtParam)\n",
    "  score = 0\n",
    "  for i in range(len(seq)-1):\n",
    "    score += diwv_dict[seq[i]][seq[i+1]]\n",
    "    score = 10.0/len(seq) * score\n",
    "  if score > 40.0:\n",
    "    return False\n",
    "\n",
    "  # ensures optimal sequence function using oracles\n",
    "  o = oracle\n",
    "  if o.predict(oracle_name, seq):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aQZay6IsVxPT"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "def run_RNN(seq_length=100, BATCH_SIZE=64, BUFFER_SIZE=10000,\n",
    "            embedding_dim=256, rnn_units=1024, num_generate=300,\n",
    "            temperature=0.1, seed=u\"MNFPRASRLMQAAVL\",\n",
    "            path_to_file=\"./seqdump_processed.txt\"):\n",
    "  # Read, then decode for py2 compat.\n",
    "  text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "  # length of text is the number of characters in it\n",
    "  print ('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "  vocab = sorted(set(text))\n",
    "  print('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "  char2idx = {c:i for i,c in enumerate(vocab)}\n",
    "  idx2char = np.array(vocab)\n",
    "\n",
    "  text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "  print('{} ---map---> {}'.format(text[:13], text_as_int[:13]))\n",
    "\n",
    "  \n",
    "  examples_per_epoch = len(text) // seq_length\n",
    "\n",
    "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "  sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "  def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "  dataset = sequences.map(split_input_target)\n",
    "\n",
    "  for input_example, target_example in dataset.take(1):\n",
    "    print('Input: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target: ', repr(''.join(idx2char[target_example.numpy()])))\n",
    "\n",
    "\n",
    "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "  dataset\n",
    "\n",
    "  vocab_size = len(vocab)\n",
    "\n",
    "  def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, \n",
    "                             stateful=True, \n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "  model = build_model(\n",
    "      vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    "  )\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "#   for input_example_batch, target_example_batch in dataset.take(1):\n",
    "#     example_batch_predictions = model(input_example_batch)\n",
    "#     print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "#   sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "#   sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "#   print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
    "#   print(\"Output: \\n\", repr(''.join(idx2char[sampled_indices])))   # powerful np.array indexing ability\n",
    "\n",
    "  def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "#   example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "#   print(\"Prediction shape: \", example_batch_loss.shape, \"# (batch_size, sequence_length)\")\n",
    "#   print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "\n",
    "  model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "  checkpoint_dir = './training_checkpoints'\n",
    "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_prefix,\n",
    "      save_weights_only=True)\n",
    "  EPOCHS=30\n",
    "\n",
    "\n",
    "  history = model.fit(dataset, epochs=EPOCHS, \n",
    "                      callbacks=[checkpoint_callback])\n",
    "\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "  model.summary()\n",
    "\n",
    "  def generate_text():\n",
    "    \n",
    "    input_eval = [char2idx[c] for c in seed]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (seed + ''.join(text_generated))\n",
    "  \n",
    "  valid_seqs = []\n",
    "  while len(valid_seqs) < 5:\n",
    "    blabble = generate_text()\n",
    "    seq = blabble.split(\"\\n\")[0]\n",
    "    if not passes_filters(seq.strip()):\n",
    "      continue\n",
    "    else:\n",
    "      valid_seqs.append(seq)\n",
    "      \n",
    "  return valid_seqs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3ZU3E4oVV0pn"
   },
   "outputs": [],
   "source": [
    "def refresh_training_set(iter_num):\n",
    "  valid_seqs = run_RNN(iter_num)\n",
    "  \n",
    "  with open(\"seqdump_processed_\" + str(iter_num) + \".txt\", \"r\") as f:\n",
    "    with open(\"seqdump_processed_\" + str(iter_num+1) + \".txt\", \"w\") as g:\n",
    "      prev_seqs = f.readlines().strip()\n",
    "      new_seqs = prev_seqs + \"\\n\\n\"\n",
    "      for seq in valid_seqs:\n",
    "        new_seqs += seq + \"\\n\\n\"\n",
    "      g.write(new_seqs)\n",
    "\n",
    "for i in range(10):\n",
    "  refresh_training_set(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFJe7cblQHQ8"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGFWZd2pa7Z1"
   },
   "source": [
    "You could use your own data flow as long as you ensure that the data format is obeyed. Alternatively, you can use the data flow we've implemented for UniRep training, which happens in the tensorflow graph. It reads from a file of integer sequences, shuffles them around, collects them into groups of similar length (to minimize padding waste) and pads them to the max_length. Here's how to do that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Z8YpNgca7Z4"
   },
   "source": [
    "First, sequences need to be saved in the correct format. Suppose we have a new-line seperated file of amino acid sequences, `seqs.txt`, and we want to format them. Note that training is currently only publicly supported for amino acid sequences less than 275 amino acids as gradient updates for sequences longer than that start to get unwieldy. If you want to train on sequences longer than this, please reach out to us. \n",
    "\n",
    "Sequence formatting can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GeOCG-Fha7Z6"
   },
   "outputs": [],
   "source": [
    "# Before you can train your model, \n",
    "with open(\"seqs.txt\", \"r\") as source:\n",
    "    with open(\"formatted.txt\", \"w\") as destination:\n",
    "        for i,seq in enumerate(source):\n",
    "            seq = seq.strip()\n",
    "            if b.is_valid_seq(seq) and len(seq) < 275: \n",
    "                formatted = \",\".join(map(str,b.format_seq(seq)))\n",
    "                destination.write(formatted)\n",
    "                destination.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gpiq1fEa7aS"
   },
   "source": [
    "This is what the integer format looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po0M7nO9a7aw",
    "outputId": "b5dfa794-e589-4d6a-d16c-28ecaade1ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24,1,2,4,13,6,6,21,18,8,13,16,16,14,17,21,16,6,21,5,13,5,16,9,13,3,4,18,7,16,2,13,6,13,6,13,5,15,8,9,13,4,21,8,21,4,18,17,11,8,8,13,4,21,14,16,14,20,14,8,21,16,8,8,21,8,19,13,16,10,11,18,15,2,19,14,5,3,1,4,10,3,5,18,18,4,7,15,1,14,6,13,19,16,10,6,2,8,17,7,18,4,5,5,13,8,19,4,8,2,15,6,16,4,18,6,13,5,8,21,16,9,2,17,6,21,4,13,17,5,18,4,6,5,13,9,17,21,13,3,4,21,6,19,9,18,9,7,3,9,16,19,17,8,15,5,4,10,4,9,13,17,4,15,9,18,4,17,2,3,9,16,6,5,13,7,16,10,21,15,5,3,19,10,10,9,8,14,17,13,5,13,14,16,21,21,14,5,9,3,19,21,7,8,10,7,16,21,7,4,5,14,9,6,4,2,5,3,1,16,21,21,6,18,16,8,15,15,13,17,8,3,13,1,5,6,21,19,4\r\n"
     ]
    }
   ],
   "source": [
    "!head -n1 formatted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onrt5SL3a7bI"
   },
   "source": [
    "Notice that by default format_seq does not include the stop symbol (25) at the end of the sequence. This is the correct behavior if you are trying to train a top model, but not if you are training UniRep representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nARMIwfMa7bK"
   },
   "source": [
    "Now we can use a custom function to bucket, batch and pad sequences from `formatted.txt` (which has the correct integer codex after calling `babbler.format_seq()`). The bucketing occurs in the graph. \n",
    "\n",
    "What is bucketing? Specify a lower and upper bound, and interval. All sequences less than lower or greater than upper will be batched together. The interval defines the \"sides\" of buckets between these bounds. Don't pick a small interval for a small dataset because the function will just repeat a sequence if there are not enough to\n",
    "fill a batch. All batches are the size you passed when initializing the babbler.\n",
    "\n",
    "This is also doing a few other things:\n",
    "- Shuffling the sequences by randomly sampling from a 10000 sequence buffer\n",
    "- Automatically padding the sequences with zeros so the returned batch is a perfect rectangle\n",
    "- Automatically repeating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GKWvFzjNa7bL"
   },
   "outputs": [],
   "source": [
    "bucket_op = b.bucket_batch_pad(\"formatted.txt\", interval=1000) # Large interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Mqtufawa7bQ"
   },
   "source": [
    "Inconveniently, this does not make it easy for a value to be associated with each sequence and not lost during shuffling. You can get around this by just prepending every integer sequence with the sequence label (eg, every sequence would be saved to the file as \"{brightness value}, 24, 1, 5,...\" and then you could just index out the first column after calling the `bucket_op`. Please reach out if you have questions on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-rdYAi0a7bS"
   },
   "source": [
    "Now that we have the `bucket_op`, we can simply `sess.run()` it to get a correctly formatted batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_q3lkLIa7be",
    "outputId": "a61208dc-f4d3-4cbf-9ed3-e62e156f6a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  1  6 ... 13 18  6]\n",
      " [24  1  7 ...  0  0  0]\n",
      " [24  1  7 ...  0  0  0]\n",
      " ...\n",
      " [24  1  4 ...  0  0  0]\n",
      " [24  1 10 ...  0  0  0]\n",
      " [24  1 13 ...  0  0  0]]\n",
      "(12, 265)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch = sess.run(bucket_op)\n",
    "    \n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6k8N1cAa7bm"
   },
   "source": [
    "You can look back and see that the batch_size we passed to __init__ is indeed 12, and the second dimension must be the longest sequence included in this batch. Now we have the data flow setup (note that as long as your batch looks like this, you don't need my flow), so we can proceed to implementing the graph. The module returns all the operations needed to feed in sequence and get out trainable representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_g-s2oja7bo"
   },
   "source": [
    "## Training a top model and a top model + mLSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDkCK5eJa7bq"
   },
   "source": [
    "First, obtain all of the ops needed to output a representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "79Nk9Ceha7bs"
   },
   "outputs": [],
   "source": [
    "final_hidden, x_placeholder, batch_size_placeholder, seq_length_placeholder, initial_state_placeholder = (\n",
    "    b.get_rep_ops())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTFU65-pa7ci"
   },
   "source": [
    "`final_hidden` should be a batch_size x rep_dim matrix.\n",
    "\n",
    "Lets say we want to train a basic feed-forward network as the top model, doing regression with MSE loss, and the Adam optimizer. We can do that by:\n",
    "\n",
    "1.  Defining a loss function.\n",
    "\n",
    "2.  Defining an optimizer that's only optimizing variables in the top model.\n",
    "\n",
    "3.  Minimizing the loss inside of a TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U9adkwyqa7ci"
   },
   "outputs": [],
   "source": [
    "y_placeholder = tf.placeholder(tf.float32, shape=[None,1], name=\"y\")\n",
    "initializer = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "\n",
    "with tf.variable_scope(\"top\"):\n",
    "    prediction = tf.contrib.layers.fully_connected(\n",
    "        final_hidden, 1, activation_fn=None, \n",
    "        weights_initializer=initializer,\n",
    "        biases_initializer=tf.zeros_initializer()\n",
    "    )\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_placeholder, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6s2wds1a7co"
   },
   "source": [
    "You can specifically train the top model first by isolating variables of the \"top\" scope, and forcing the optimizer to only optimize these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YNAQqPPca7dE"
   },
   "outputs": [],
   "source": [
    "learning_rate=.001\n",
    "top_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"top\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "top_only_step_op = optimizer.minimize(loss, var_list=top_variables)\n",
    "all_step_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfSKUEFya7dJ"
   },
   "source": [
    "We next need to define a function that allows us to calculate the length each sequence in the batch so that we know what index to use to obtain the right \"final\" hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fAky6s4a7dR",
    "outputId": "46390615-1465-407c-c456-9b646521007c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([265, 195, 239, 176, 230, 251,  97, 223, 148, 223, 248, 205])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonpad_len(batch):\n",
    "    nonzero = batch > 0\n",
    "    lengths = np.sum(nonzero, axis=1)\n",
    "    return lengths\n",
    "\n",
    "nonpad_len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-l-Qw1wa7e7"
   },
   "source": [
    "We are ready to train. As an illustration, let's learn to predict the number 42 just optimizing the top model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c29Cp3--a7e-",
    "outputId": "c55e78b8-0957-403f-be3b-7833cf6341ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 1757.400390625\n",
      "Iteration 1: 1763.5225830078125\n",
      "Iteration 2: 1736.2425537109375\n",
      "Iteration 3: 1744.5452880859375\n",
      "Iteration 4: 1726.3717041015625\n",
      "Iteration 5: 1711.666015625\n",
      "Iteration 6: 1719.5970458984375\n",
      "Iteration 7: 1693.2880859375\n",
      "Iteration 8: 1701.9228515625\n",
      "Iteration 9: 1683.3359375\n"
     ]
    }
   ],
   "source": [
    "y = [[42]]*batch_size\n",
    "num_iters = 10\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_iters):\n",
    "        batch = sess.run(bucket_op)\n",
    "        length = nonpad_len(batch)\n",
    "        loss_, __, = sess.run([loss, top_only_step_op],\n",
    "                feed_dict={\n",
    "                     x_placeholder: batch,\n",
    "                     y_placeholder: y,\n",
    "                     batch_size_placeholder: batch_size,\n",
    "                     seq_length_placeholder:length,\n",
    "                     initial_state_placeholder:b._zero_state\n",
    "                }\n",
    "        )\n",
    "                  \n",
    "        print(\"Iteration {0}: {1}\".format(i, loss_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dO5D_dUxa7fO"
   },
   "source": [
    "We can also jointly train the top model and the mLSTM. Note that if using the 1900-unit (full) model, you will need a GPU with at least 16GB RAM. To see a demonstration of joint training with fewer computational resources, please run this notebook using the 64-unit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIiC3PXUa7fQ",
    "outputId": "b57f5181-2d56-43d4-dd27-b0f1dd40f626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 1757.400390625\n",
      "Iteration 1: 1720.1832275390625\n",
      "Iteration 2: 1650.4283447265625\n",
      "Iteration 3: 1552.2196044921875\n",
      "Iteration 4: 1369.4599609375\n",
      "Iteration 5: 1174.6700439453125\n",
      "Iteration 6: 981.3558959960938\n",
      "Iteration 7: 798.6965942382812\n",
      "Iteration 8: 715.9775390625\n",
      "Iteration 9: 580.2219848632812\n"
     ]
    }
   ],
   "source": [
    "y = [[42]]*batch_size\n",
    "num_iters = 10\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_iters):\n",
    "        batch = sess.run(bucket_op)\n",
    "        length = nonpad_len(batch)\n",
    "        loss_, __, = sess.run([loss, all_step_op],\n",
    "                feed_dict={\n",
    "                     x_placeholder: batch,\n",
    "                     y_placeholder: y,\n",
    "                     batch_size_placeholder: batch_size,\n",
    "                     seq_length_placeholder:length,\n",
    "                     initial_state_placeholder:b._zero_state\n",
    "                }\n",
    "        )\n",
    "        \n",
    "        print(\"Iteration {0}: {1}\".format(i,loss_))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "protein_design.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
